{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4d391ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18e48230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36bb2f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alireza/miniconda3/envs/NLP_project/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "def extract_docs():\n",
    "    from datasets import load_dataset\n",
    "    xsum = load_dataset(\"EdinburghNLP/xsum\")  # :contentReference[oaicite:1]{index=1}\n",
    "    docs = {ex[\"id\"]: ex[\"document\"] for split in xsum.values()  # :contentReference[oaicite:2]{index=2}\n",
    "            for ex in split}\n",
    "    return docs\n",
    "import pandas as pd\n",
    "factual_df = pd.read_csv(\"../Data/factuality_annotations_xsum_summaries.csv\")\n",
    "\n",
    "docs = extract_docs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5329117e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(text: str):\n",
    "    \"\"\"\n",
    "    Returns the list of entity strings found in text.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    return [ent.text for ent in doc.ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49a8d9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_svo_triples(text: str):\n",
    "#     \"\"\"\n",
    "#     Returns list of (subject, verb, object) triples from text.\n",
    "#     \"\"\"\n",
    "#     doc = nlp(text)\n",
    "#     triples = []\n",
    "#     for token in doc:\n",
    "#         # look for subjects of verbs\n",
    "#         if token.dep_ == \"nsubj\" and token.head.pos_ == \"VERB\":\n",
    "#             subj = token.text\n",
    "#             verb = token.head.lemma_\n",
    "#             # find direct objects of that verb\n",
    "#             objs = [child.text for child in token.head.children\n",
    "#                     if child.dep_ in (\"dobj\", \"obj\", \"attr\")]\n",
    "#             for obj in objs:\n",
    "#                 triples.append((subj, verb, obj))\n",
    "#     return triples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa669ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def extract_svo_triples(text: str):\n",
    "#     \"\"\"\n",
    "#     Enhanced S–V–O extractor:\n",
    "#       • Iterates over all VERB tokens (including conj children)\n",
    "#       • Finds subjects via nsubj / nsubjpass\n",
    "#       • Finds objects via dobj, pobj, attr, or prep→pobj\n",
    "#       • Ignores pronoun subjects/objects\n",
    "#     \"\"\"\n",
    "#     doc = nlp(text)\n",
    "#     triples = []\n",
    "\n",
    "#     for token in doc:\n",
    "#         # consider any verb or its conjuncts\n",
    "#         if token.pos_ != \"VERB\":\n",
    "#             continue\n",
    "\n",
    "#         # gather subjects\n",
    "#         subjects = [\n",
    "#             child for child in token.children\n",
    "#             if child.dep_ in (\"nsubj\", \"nsubjpass\") and child.pos_ != \"PRON\"\n",
    "#         ]\n",
    "#         if not subjects:\n",
    "#             continue\n",
    "\n",
    "#         # gather direct objects and attribute objects\n",
    "#         objects = [\n",
    "#             child for child in token.children\n",
    "#             if child.dep_ in (\"dobj\", \"attr\") and child.pos_ != \"PRON\"\n",
    "#         ]\n",
    "\n",
    "#         # gather objects via prepositions (prep → pobj)\n",
    "#         for prep in [c for c in token.children if c.dep_ == \"prep\"]:\n",
    "#             for pobj in [c2 for c2 in prep.children if c2.dep_ == \"pobj\" and c2.pos_ != \"PRON\"]:\n",
    "#                 objects.append(pobj)\n",
    "\n",
    "#         # if no direct or prep objects, skip\n",
    "#         if not objects:\n",
    "#             continue\n",
    "\n",
    "#         # record every combination\n",
    "#         for subj in subjects:\n",
    "#             for obj in objects:\n",
    "#                 triples.append((subj.text, token.lemma_, obj.text))\n",
    "\n",
    "#     return triples\n",
    "\n",
    "\n",
    "# src = docs[str(factual_df.loc[10, \"bbcid\"])]\n",
    "\n",
    "# for sent in nlp(src).sents:\n",
    "#     print(sent.text)\n",
    "#     print(\"→\", extract_svo_triples(sent.text))\n",
    "#     print()\n",
    "\n",
    "\n",
    "# # src   = docs[str(factual_df['bbcid'][10])]\n",
    "# # summ  = factual_df['summary'][10]\n",
    "# # print(src)\n",
    "# # extract_svo_triples(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ea5c114d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "class Phrases():\n",
    "    def __init__(self, sentence):\n",
    "       self.nlp = spacy.load('en_core_web_sm')\n",
    "       self.sentence = str(sentence)\n",
    "       self.doc = self.nlp(self.sentence)\n",
    "       self.sequence = 0\n",
    "       self.svos = []\n",
    "\n",
    "    def merge_phrases(self):\n",
    "        with self.doc.retokenize() as retokenizer:\n",
    "            for np in list(self.doc.noun_chunks):\n",
    "                    attrs = {\n",
    "                        \"tag\": np.root.tag_,\n",
    "                        \"lemma\": np.root.lemma_,\n",
    "                        \"ent_type\": np.root.ent_type_,\n",
    "                    }\n",
    "                    retokenizer.merge(np, attrs=attrs)\n",
    "        return self.doc\n",
    "\n",
    "    def merge_punct(self):\n",
    "        spans = []\n",
    "        for word in self.doc[:-1]:\n",
    "            if word.is_punct or not word.nbor(1).is_punct:\n",
    "                continue\n",
    "            start = word.i\n",
    "            end = word.i + 1\n",
    "            while end < len(self.doc) and self.doc[end].is_punct:\n",
    "                end += 1\n",
    "            span = self.doc[start:end]\n",
    "            spans.append((span, word.tag_, word.lemma_, word.ent_type_))\n",
    "        with self.doc.retokenize() as retokenizer:\n",
    "            for span, tag, lemma, ent_type in spans:\n",
    "                attrs = {\"tag\": tag, \"lemma\": lemma, \"ent_type\": ent_type}\n",
    "                retokenizer.merge(span, attrs=attrs)\n",
    "        return self.doc\n",
    "\n",
    "    def is_passive(self, tokens):\n",
    "        for tok in tokens:\n",
    "            if tok.dep_ == \"auxpass\":\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def _is_verb(self, token):\n",
    "        return token.dep_ in [\"ROOT\", \"xcomp\", \"appos\", \"advcl\", \"ccomp\", \"conj\"] and token.tag_ in [\"VB\", \"VBZ\", \"VBD\", \"VBN\", \"VBG\", \"VBP\"]\n",
    "\n",
    "    def find_verbs(self, tokens):\n",
    "        verbs = [tok for tok in tokens if self._is_verb(tok)]\n",
    "        return verbs\n",
    "\n",
    "    def get_all_subs(self, v):\n",
    "        #get all subjects\n",
    "        subs = [tok for tok in v.lefts if tok.dep_ in [\"ROOT\", \"nsubj\", \"nsubjpass\"] and tok.tag_ in [\"NN\" , \"NNS\", \"NNP\"]]\n",
    "        if len(subs) == 0:\n",
    "            #get all subjects from the left of verb (\"nsubj\" <= \"preconj\" <= VERB)\n",
    "            subs = [tok for tok in v.lefts if tok.dep_ in [\"preconj\"]]\n",
    "            for sub in subs:\n",
    "                rights = list(sub.rights)\n",
    "                right_dependency = [tok.lower_ for tok in rights]\n",
    "                if len(right_dependency) > 0:\n",
    "                    subs = right_dependency[0]\n",
    "        return subs\n",
    "\n",
    "    def get_all_objs(self, v, is_pas):\n",
    "        #get list the right of dependency with VERB (VERB => \"dobj\" or \"pobj\")\n",
    "        rights = list(v.rights)\n",
    "        objs = [tok for tok in rights if tok.dep_ in [\"dobj\", \"dative\", \"attr\", \"oprd\", \"pobj\"] or (is_pas and tok.dep_ == 'pobj')]\n",
    "        #get all objects from the right of dependency (VERB => \"dobj\" or \"pobj\")\n",
    "        for obj in objs:\n",
    "            #on the right of dependency, you can get objects from prepositions (VERB => \"dobj\" => \"prep\" => \"pobj\")\n",
    "            rights = list(obj.rights) \n",
    "            objs.extend(self._get_objs_from_prepositions(rights, is_pas))\n",
    "        return v, objs\n",
    "\n",
    "    def _get_objs_from_prepositions(self, deps, is_pas):\n",
    "        objs = []\n",
    "        for dep in deps:\n",
    "            if dep.pos_ == \"ADP\" and (dep.dep_ == \"prep\" or (is_pas and dep.dep_ == \"agent\")):\n",
    "                objs.extend([tok for tok in dep.rights if tok.dep_  in [\"dobj\", \"dative\", \"attr\", \"oprd\", \"pobj\"] or (tok.pos_ == \"PRON\" and tok.lower_ == \"me\") or (is_pas and tok.dep_ == 'pobj')])\n",
    "        return objs\n",
    "\n",
    "    def main_get_to_pobj(self, verb):\n",
    "        for tok in verb.rights:\n",
    "            if tok.dep_ == \"prep\" and tok.lower_ == \"to\":\n",
    "                for child in tok.children:\n",
    "                    if child.dep_ == \"pobj\":\n",
    "                        return child.text\n",
    "        return None \n",
    "\n",
    "    def get_svo(self, sentence):\n",
    "        doc = self.nlp(sentence)\n",
    "        doc = self.merge_phrases()\n",
    "        doc = self.merge_phrases()\n",
    "\n",
    "        #check passive and active sentence\n",
    "        is_pas = self.is_passive(doc)\n",
    "\n",
    "        #find the main verb and child of a verb\n",
    "        verbs = self.find_verbs(doc) \n",
    "\n",
    "        #more than verb\n",
    "        for verb in verbs:\n",
    "            self.sequence += 1\n",
    "\n",
    "            #find the subject with the main verb\n",
    "            subject = self.get_all_subs(verb)\n",
    "\n",
    "            #find the object with the main verb                \n",
    "            verb, obj = self.get_all_objs(verb, is_pas)\n",
    "\n",
    "            #find prepositional modifier with the main verb  \n",
    "            to_pobj = self.main_get_to_pobj(verb)\n",
    "\n",
    "            #You can continue create method for extract word ...\n",
    "\n",
    "            #finally, we can find prepositional modifier with the main verb\n",
    "            if to_pobj is not None:\n",
    "                #result SVO\n",
    "                self.svos.append((self.sequence, subject, verb, obj, to_pobj))\n",
    "            else:\n",
    "                #result SVO\n",
    "                self.svos.append((self.sequence, subject, verb, obj, \"\"))\n",
    "        \n",
    "        return self.svos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4096ba51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy model once at the module level\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def merge_phrases(doc):\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for np in list(doc.noun_chunks):\n",
    "            attrs = {\n",
    "                \"tag\": np.root.tag_,\n",
    "                \"lemma\": np.root.lemma_,\n",
    "                \"ent_type\": np.root.ent_type_,\n",
    "            }\n",
    "            retokenizer.merge(np, attrs=attrs)\n",
    "    return doc\n",
    "\n",
    "def merge_punct(doc):\n",
    "    spans = []\n",
    "    for word in doc[:-1]:\n",
    "        if word.is_punct or not word.nbor(1).is_punct:\n",
    "            continue\n",
    "        start = word.i\n",
    "        end = word.i + 1\n",
    "        while end < len(doc) and doc[end].is_punct:\n",
    "            end += 1\n",
    "        span = doc[start:end]\n",
    "        spans.append((span, word.tag_, word.lemma_, word.ent_type_))\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for span, tag, lemma, ent_type in spans:\n",
    "            attrs = {\"tag\": tag, \"lemma\": lemma, \"ent_type\": ent_type}\n",
    "            retokenizer.merge(span, attrs=attrs)\n",
    "    return doc\n",
    "\n",
    "def is_passive(doc):\n",
    "    return any(tok.dep_ == \"auxpass\" for tok in doc)\n",
    "\n",
    "def is_verb(tok):\n",
    "    return tok.dep_ in [\"ROOT\", \"xcomp\", \"appos\", \"advcl\", \"ccomp\", \"conj\"] and tok.tag_ in [\"VB\", \"VBZ\", \"VBD\", \"VBN\", \"VBG\", \"VBP\"]\n",
    "\n",
    "def find_verbs(doc):\n",
    "    return [tok for tok in doc if is_verb(tok)]\n",
    "\n",
    "def get_all_subs(verb):\n",
    "    subs = [tok for tok in verb.lefts if tok.dep_ in [\"ROOT\", \"nsubj\", \"nsubjpass\"] and tok.tag_ in [\"NN\", \"NNS\", \"NNP\"]]\n",
    "    if not subs:\n",
    "        subs = [tok for tok in verb.lefts if tok.dep_ == \"preconj\"]\n",
    "        for sub in subs:\n",
    "            rights = list(sub.rights)\n",
    "            if rights:\n",
    "                return [rights[0]]\n",
    "    return subs\n",
    "\n",
    "def _get_objs_from_prepositions(deps, is_pas):\n",
    "    objs = []\n",
    "    for dep in deps:\n",
    "        if dep.pos_ == \"ADP\" and (dep.dep_ == \"prep\" or (is_pas and dep.dep_ == \"agent\")):\n",
    "            objs.extend([tok for tok in dep.rights if tok.dep_ in [\"dobj\", \"dative\", \"attr\", \"oprd\", \"pobj\"]\n",
    "                         or (tok.pos_ == \"PRON\" and tok.lower_ == \"me\")\n",
    "                         or (is_pas and tok.dep_ == 'pobj')])\n",
    "    return objs\n",
    "\n",
    "def get_all_objs(verb, is_pas):\n",
    "    rights = list(verb.rights)\n",
    "    objs = [tok for tok in rights if tok.dep_ in [\"dobj\", \"dative\", \"attr\", \"oprd\", \"pobj\"] or (is_pas and tok.dep_ == 'pobj')]\n",
    "    for obj in objs:\n",
    "        rights = list(obj.rights)\n",
    "        objs.extend(_get_objs_from_prepositions(rights, is_pas))\n",
    "    return objs\n",
    "\n",
    "def main_get_to_pobj(verb):\n",
    "    for tok in verb.rights:\n",
    "        if tok.dep_ == \"prep\" and tok.lower_ == \"to\":\n",
    "            for child in tok.children:\n",
    "                if child.dep_ == \"pobj\":\n",
    "                    return child.text\n",
    "    return None\n",
    "\n",
    "def extract_svo(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    doc = merge_phrases(doc)\n",
    "    doc = merge_phrases(doc)  # twice, as in original logic\n",
    "\n",
    "    is_pas = is_passive(doc)\n",
    "    verbs = find_verbs(doc)\n",
    "\n",
    "    svos = []\n",
    "    sequence = 0\n",
    "\n",
    "    for verb in verbs:\n",
    "        sequence += 1\n",
    "        subs = get_all_subs(verb)\n",
    "        objs = get_all_objs(verb, is_pas)\n",
    "        to_pobj = main_get_to_pobj(verb)\n",
    "\n",
    "        # convert tokens to text\n",
    "        sub_text = [tok.text for tok in subs]\n",
    "        obj_text = [tok.text for tok in objs]\n",
    "        verb_text = verb.lemma_  # canonical verb form\n",
    "\n",
    "        svos.append((sequence, sub_text, verb_text, obj_text, to_pobj or \"\"))\n",
    "\n",
    "    return svos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b56d00e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Former Dons midfielder Sheerin, 39, has been player-manager at the Red Lichties since 2010 and replaces Neil Cooper at Pittodrie.\n",
      "Arbroath were relegated to Scottish League Two after finishing season 2013-14 bottom of League One.\n",
      "Aberdeen chief executive Duncan Fraser said boss Derek McInnes chose Sheerin after \"a comprehensive process\".\n",
      "Speaking on the club's website, Fraser added: \"Despite wishing to retain Paul's services, Arbroath chairman John Christison was good to deal with and completely appreciated Paul's desire to come back to Aberdeen.\"\n",
      "Sheerin played for several Scottish clubs and also featured for Östersunds in Sweden before moving into coaching. He won a Scotland Under-21 cap in in the mid-1990s.\n",
      "Sheerin guided the Arbroath to their first national trophy in their history by winning the Scottish Third Division title and promotion in 2011.\n",
      "[1] SUBJECT: ['Former Dons midfielder Sheerin'] | VERB: be | OBJECT: ['player-manager', 'the Red Lichties'] | TO_POBJ: \n",
      "[2] SUBJECT: [] | VERB: replace | OBJECT: ['Neil Cooper'] | TO_POBJ: \n",
      "[3] SUBJECT: ['Arbroath'] | VERB: relegate | OBJECT: [] | TO_POBJ: Scottish League\n",
      "[4] SUBJECT: ['Aberdeen chief executive Duncan Fraser'] | VERB: say | OBJECT: [] | TO_POBJ: \n",
      "[5] SUBJECT: ['boss Derek McInnes'] | VERB: choose | OBJECT: ['Sheerin'] | TO_POBJ: \n",
      "[6] SUBJECT: [] | VERB: speak | OBJECT: [] | TO_POBJ: \n",
      "[7] SUBJECT: ['Fraser'] | VERB: add | OBJECT: [] | TO_POBJ: \n",
      "[8] SUBJECT: [] | VERB: retain | OBJECT: [\"Paul's services\"] | TO_POBJ: \n",
      "[9] SUBJECT: ['Arbroath chairman John Christison'] | VERB: be | OBJECT: [] | TO_POBJ: \n",
      "[10] SUBJECT: [] | VERB: deal | OBJECT: [] | TO_POBJ: \n",
      "[11] SUBJECT: [] | VERB: appreciate | OBJECT: [\"Paul's desire\"] | TO_POBJ: \n",
      "[12] SUBJECT: ['Sheerin'] | VERB: play | OBJECT: [] | TO_POBJ: \n",
      "[13] SUBJECT: [] | VERB: feature | OBJECT: [] | TO_POBJ: \n",
      "[14] SUBJECT: [] | VERB: win | OBJECT: ['a Scotland Under-21 cap'] | TO_POBJ: \n",
      "[15] SUBJECT: ['Sheerin'] | VERB: guide | OBJECT: ['the Arbroath'] | TO_POBJ: their first national trophy\n"
     ]
    }
   ],
   "source": [
    "src = docs[str(factual_df.loc[10, \"bbcid\"])]\n",
    "print(src)\n",
    "\n",
    "result = extract_svo( src)\n",
    "for r in result:\n",
    "    print(f\"[{r[0]}] SUBJECT: {r[1]} | VERB: {r[2]} | OBJECT: {r[3]} | TO_POBJ: {r[4]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396f08d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2852a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, [ali], went, [], 'the bathroom')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.get_svo('ali went to the bathroom.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7cb6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq, sub, verb, obj, to_pobj in svos:\n",
    "    sub_text = [tok.text if hasattr(tok, 'text') else str(tok) for tok in sub]\n",
    "    verb_text = verb.lemma_ if hasattr(verb, 'lemma_') else str(verb)\n",
    "    obj_text = [tok.text if hasattr(tok, 'text') else str(tok) for tok in obj]\n",
    "    print(f\"[{seq}] SUBJECT: {sub_text} | VERB: {verb_text} | OBJECT: {obj_text} | TO_POBJ: {to_pobj}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63431df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Former Dons midfielder Sheerin, 39, has been player-manager at the Red Lichties since 2010 and replaces Neil Cooper at Pittodrie.\n",
      "\n",
      "Token Former POS: ADJ, dep: amod\n",
      "Token Dons POS: PROPN, dep: compound\n",
      "Token midfielder POS: NOUN, dep: compound\n",
      "Token Sheerin POS: PROPN, dep: nsubj\n",
      "Token , POS: PUNCT, dep: punct\n",
      "Token 39 POS: NUM, dep: appos\n",
      "Token , POS: PUNCT, dep: punct\n",
      "Token has POS: AUX, dep: aux\n",
      "Token been POS: AUX, dep: ROOT\n",
      "Token player POS: NOUN, dep: compound\n",
      "Token - POS: PUNCT, dep: punct\n",
      "Token manager POS: NOUN, dep: attr\n",
      "Token at POS: ADP, dep: prep\n",
      "Token the POS: DET, dep: det\n",
      "Token Red POS: PROPN, dep: compound\n",
      "Token Lichties POS: PROPN, dep: pobj\n",
      "Token since POS: SCONJ, dep: prep\n",
      "Token 2010 POS: NUM, dep: pobj\n",
      "Token and POS: CCONJ, dep: cc\n",
      "Token replaces POS: VERB, dep: conj\n",
      "Token Neil POS: PROPN, dep: compound\n",
      "Token Cooper POS: PROPN, dep: dobj\n",
      "Token at POS: ADP, dep: prep\n",
      "Token Pittodrie POS: PROPN, dep: pobj\n",
      "Token . POS: PUNCT, dep: punct\n",
      "Token \n",
      " POS: SPACE, dep: dep\n",
      "→ ('midfielder sheerin , 39 ,', 'replaces', 'player - manager at neil cooper')\n",
      "svo:, subject: midfielder sheerin , 39 ,, verb: replaces, attribute: player - manager at neil cooper, question: is_question(doc), wh_word: is_question(doc)\n",
      "\n",
      "Arbroath were relegated to Scottish League Two after finishing season 2013-14 bottom of League One.\n",
      "\n",
      "Token Arbroath POS: PROPN, dep: nsubjpass\n",
      "Token were POS: AUX, dep: auxpass\n",
      "Token relegated POS: VERB, dep: ROOT\n",
      "Token to POS: ADP, dep: prep\n",
      "Token Scottish POS: PROPN, dep: compound\n",
      "Token League POS: PROPN, dep: pobj\n",
      "Token Two POS: PROPN, dep: npadvmod\n",
      "Token after POS: ADP, dep: prep\n",
      "Token finishing POS: VERB, dep: pcomp\n",
      "Token season POS: NOUN, dep: nmod\n",
      "Token 2013 POS: NUM, dep: nummod\n",
      "Token - POS: SYM, dep: punct\n",
      "Token 14 POS: NUM, dep: prep\n",
      "Token bottom POS: NOUN, dep: dobj\n",
      "Token of POS: ADP, dep: prep\n",
      "Token League POS: PROPN, dep: compound\n",
      "Token One POS: PROPN, dep: pobj\n",
      "Token . POS: PUNCT, dep: punct\n",
      "Token \n",
      " POS: SPACE, dep: dep\n",
      "→ ('arbroath', 'relegated finishing', 'season 2013 bottom of')\n",
      "svo:, subject: arbroath, verb: relegated finishing, attribute: season 2013 bottom of, question: is_question(doc), wh_word: is_question(doc)\n",
      "\n",
      "Aberdeen chief executive Duncan Fraser said boss Derek McInnes chose Sheerin after \"a comprehensive process\".\n",
      "\n",
      "Token Aberdeen POS: PROPN, dep: compound\n",
      "Token chief POS: ADJ, dep: amod\n",
      "Token executive POS: NOUN, dep: compound\n",
      "Token Duncan POS: PROPN, dep: compound\n",
      "Token Fraser POS: PROPN, dep: nsubj\n",
      "Token said POS: VERB, dep: ROOT\n",
      "Token boss POS: NOUN, dep: compound\n",
      "Token Derek POS: PROPN, dep: compound\n",
      "Token McInnes POS: PROPN, dep: nsubj\n",
      "Token chose POS: VERB, dep: ccomp\n",
      "Token Sheerin POS: PROPN, dep: dobj\n",
      "Token after POS: ADP, dep: prep\n",
      "Token \" POS: PUNCT, dep: punct\n",
      "Token a POS: DET, dep: det\n",
      "Token comprehensive POS: ADJ, dep: amod\n",
      "Token process POS: NOUN, dep: pobj\n",
      "Token \" POS: PUNCT, dep: punct\n",
      "Token . POS: PUNCT, dep: punct\n",
      "Token \n",
      " POS: SPACE, dep: dep\n",
      "→ ('executive duncan fraser boss derek mcinnes', 'said chose', 'sheerin')\n",
      "svo:, subject: executive duncan fraser boss derek mcinnes, verb: said chose, attribute: sheerin, question: is_question(doc), wh_word: is_question(doc)\n",
      "\n",
      "Speaking on the club's website, Fraser added: \"Despite wishing to retain Paul's services, Arbroath chairman John Christison was good to deal with and completely appreciated Paul's desire to come back to Aberdeen.\"\n",
      "Sheerin played for several Scottish clubs and also featured for Östersunds in Sweden before moving into coaching.\n",
      "Token Speaking POS: VERB, dep: advcl\n",
      "Token on POS: ADP, dep: prep\n",
      "Token the POS: DET, dep: det\n",
      "Token club POS: NOUN, dep: poss\n",
      "Token 's POS: PART, dep: case\n",
      "Token website POS: NOUN, dep: pobj\n",
      "Token , POS: PUNCT, dep: punct\n",
      "Token Fraser POS: PROPN, dep: nsubj\n",
      "Token added POS: VERB, dep: ROOT\n",
      "Token : POS: PUNCT, dep: punct\n",
      "Token \" POS: PUNCT, dep: punct\n",
      "Token Despite POS: SCONJ, dep: prep\n",
      "Token wishing POS: VERB, dep: pcomp\n",
      "Token to POS: PART, dep: aux\n",
      "Token retain POS: VERB, dep: xcomp\n",
      "Token Paul POS: PROPN, dep: poss\n",
      "Token 's POS: PART, dep: case\n",
      "Token services POS: NOUN, dep: dobj\n",
      "Token , POS: PUNCT, dep: punct\n",
      "Token Arbroath POS: PROPN, dep: compound\n",
      "Token chairman POS: NOUN, dep: compound\n",
      "Token John POS: PROPN, dep: compound\n",
      "Token Christison POS: PROPN, dep: nsubj\n",
      "Token was POS: AUX, dep: ccomp\n",
      "Token good POS: ADJ, dep: acomp\n",
      "Token to POS: PART, dep: aux\n",
      "Token deal POS: VERB, dep: xcomp\n",
      "Token with POS: ADP, dep: prep\n",
      "Token and POS: CCONJ, dep: cc\n",
      "Token completely POS: ADV, dep: advmod\n",
      "Token appreciated POS: VERB, dep: conj\n",
      "Token Paul POS: PROPN, dep: poss\n",
      "Token 's POS: PART, dep: case\n",
      "Token desire POS: NOUN, dep: dobj\n",
      "Token to POS: PART, dep: aux\n",
      "Token come POS: VERB, dep: acl\n",
      "Token back POS: ADV, dep: advmod\n",
      "Token to POS: ADP, dep: prep\n",
      "Token Aberdeen POS: PROPN, dep: pobj\n",
      "Token . POS: PUNCT, dep: punct\n",
      "Token \" POS: PUNCT, dep: punct\n",
      "Token \n",
      " POS: SPACE, dep: dep\n",
      "Token Sheerin POS: PROPN, dep: nsubj\n",
      "Token played POS: VERB, dep: ccomp\n",
      "Token for POS: ADP, dep: prep\n",
      "Token several POS: ADJ, dep: amod\n",
      "Token Scottish POS: ADJ, dep: amod\n",
      "Token clubs POS: NOUN, dep: pobj\n",
      "Token and POS: CCONJ, dep: cc\n",
      "Token also POS: ADV, dep: advmod\n",
      "Token featured POS: VERB, dep: conj\n",
      "Token for POS: ADP, dep: prep\n",
      "Token Östersunds POS: PROPN, dep: pobj\n",
      "Token in POS: ADP, dep: prep\n",
      "Token Sweden POS: PROPN, dep: pobj\n",
      "Token before POS: ADP, dep: prep\n",
      "Token moving POS: VERB, dep: pcomp\n",
      "Token into POS: ADP, dep: prep\n",
      "Token coaching POS: NOUN, dep: pobj\n",
      "Token . POS: PUNCT, dep: punct\n",
      "→ ('fraser chairman john christison sheerin', 'speaking added wishing retain deal appreciated come played featured moving', 'paul services paul desire come')\n",
      "svo:, subject: fraser chairman john christison sheerin, verb: speaking added wishing retain deal appreciated come played featured moving, attribute: paul services paul desire come, question: is_question(doc), wh_word: is_question(doc)\n",
      "\n",
      "He won a Scotland Under-21 cap in in the mid-1990s.\n",
      "\n",
      "Token He POS: PRON, dep: nsubj\n",
      "Token won POS: VERB, dep: ROOT\n",
      "Token a POS: DET, dep: det\n",
      "Token Scotland POS: PROPN, dep: compound\n",
      "Token Under-21 POS: PROPN, dep: compound\n",
      "Token cap POS: NOUN, dep: dobj\n",
      "Token in POS: ADP, dep: prt\n",
      "Token in POS: ADP, dep: prep\n",
      "Token the POS: DET, dep: det\n",
      "Token mid-1990s POS: NOUN, dep: pobj\n",
      "Token . POS: PUNCT, dep: punct\n",
      "Token \n",
      " POS: SPACE, dep: dep\n",
      "→ ('he', 'won', 'a under-21 cap')\n",
      "svo:, subject: he, verb: won, attribute: a under-21 cap, question: is_question(doc), wh_word: is_question(doc)\n",
      "\n",
      "Sheerin guided the Arbroath to their first national trophy in their history by winning the Scottish Third Division title and promotion in 2011.\n",
      "Token Sheerin POS: PROPN, dep: nsubj\n",
      "Token guided POS: VERB, dep: ROOT\n",
      "Token the POS: DET, dep: det\n",
      "Token Arbroath POS: PROPN, dep: dobj\n",
      "Token to POS: ADP, dep: prep\n",
      "Token their POS: PRON, dep: poss\n",
      "Token first POS: ADJ, dep: amod\n",
      "Token national POS: ADJ, dep: amod\n",
      "Token trophy POS: NOUN, dep: pobj\n",
      "Token in POS: ADP, dep: prep\n",
      "Token their POS: PRON, dep: poss\n",
      "Token history POS: NOUN, dep: pobj\n",
      "Token by POS: ADP, dep: prep\n",
      "Token winning POS: VERB, dep: pcomp\n",
      "Token the POS: DET, dep: det\n",
      "Token Scottish POS: PROPN, dep: compound\n",
      "Token Third POS: PROPN, dep: compound\n",
      "Token Division POS: PROPN, dep: compound\n",
      "Token title POS: NOUN, dep: dobj\n",
      "Token and POS: CCONJ, dep: cc\n",
      "Token promotion POS: NOUN, dep: conj\n",
      "Token in POS: ADP, dep: prep\n",
      "Token 2011 POS: NUM, dep: pobj\n",
      "Token . POS: PUNCT, dep: punct\n",
      "→ ('sheerin', 'guided winning', 'the arbroath the division title and promotion')\n",
      "svo:, subject: sheerin, verb: guided winning, attribute: the arbroath the division title and promotion, question: is_question(doc), wh_word: is_question(doc)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# object and subject constants\n",
    "OBJECT_DEPS = {\"dobj\", \"dative\", \"attr\", \"oprd\"}\n",
    "SUBJECT_DEPS = {\"nsubj\", \"nsubjpass\", \"csubj\", \"agent\", \"expl\"}\n",
    "# tags that define wether the word is wh-\n",
    "WH_WORDS = {\"WP\", \"WP$\", \"WRB\"}\n",
    "\n",
    "# extract the subject, object and verb from the input\n",
    "def extract_svo(doc):\n",
    "    sub = []\n",
    "    at = []\n",
    "    ve = []\n",
    "    for token in doc:\n",
    "        # is this a verb?\n",
    "        if token.pos_ == \"VERB\":\n",
    "            ve.append(token.text)\n",
    "        # is this the object?\n",
    "        if token.dep_ in OBJECT_DEPS or token.head.dep_ in OBJECT_DEPS:\n",
    "            at.append(token.text)\n",
    "        # is this the subject?\n",
    "        if token.dep_ in SUBJECT_DEPS or token.head.dep_ in SUBJECT_DEPS:\n",
    "            sub.append(token.text)\n",
    "    return \" \".join(sub).strip().lower(), \" \".join(ve).strip().lower(), \" \".join(at).strip().lower()\n",
    "\n",
    "# wether the doc is a question, as well as the wh-word if any\n",
    "def is_question(doc):\n",
    "    # is the first token a verb?\n",
    "    if len(doc) > 0 and doc[0].pos_ == \"VERB\":\n",
    "        return True, \"\"\n",
    "    # go over all words\n",
    "    for token in doc:\n",
    "        # is it a wh- word?\n",
    "        if token.tag_ in WH_WORDS:\n",
    "            return True, token.text.lower()\n",
    "    return False, \"\"\n",
    "\n",
    "\n",
    "\n",
    "src = docs[str(factual_df.loc[10, \"bbcid\"])]\n",
    "\n",
    "for sent in nlp(src).sents:\n",
    "    print(sent.text)\n",
    "    for token in sent:\n",
    "        print(\"Token {} POS: {}, dep: {}\".format(token.text, token.pos_, token.dep_))\n",
    "    print(\"→\", extract_svo(sent))\n",
    "    subject, verb, attribute =  extract_svo(sent)\n",
    "    print(\"svo:, subject: {}, verb: {}, attribute: {}, question: {}, wh_word: {}\".format(subject, verb, attribute, question, wh_word))\n",
    "    print()\n",
    "\n",
    "# doc   = docs[str(factual_df['bbcid'][10])]\n",
    "# get the input information\n",
    "# subject, verb, attribute = extract_svo(doc)\n",
    "question, wh_word = \"is_question(doc)\" , \"is_question(doc)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557163e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCE: Former Dons midfielder Sheerin, 39, has been player-manager at the Red Lichties since 2010 and replaces Neil Cooper at Pittodrie.\n",
      "→ [('Sheerin', 'be', 'player-manager at the Red Lichties'), ('Sheerin', 'replace', 'Neil Cooper'), ('Sheerin', 'replace', 'Pittodrie')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import spacy\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# def extract_svo_triples(text: str):\n",
    "#     doc = nlp(text)\n",
    "#     svos = []\n",
    "\n",
    "#     for sent in doc.sents:\n",
    "#         # 1) Identify ROOT verbs (VERB or AUX) and build subject map\n",
    "#         roots = [t for t in sent if t.dep_==\"ROOT\" and t.pos_ in (\"VERB\",\"AUX\")]\n",
    "#         subj_map = {}\n",
    "#         for root in roots:\n",
    "#             subs = [c for c in root.children\n",
    "#                     if c.dep_ in (\"nsubj\",\"nsubjpass\") and c.pos_!=\"PRON\"]\n",
    "#             subj_map[root] = subs\n",
    "\n",
    "#         # 2) Collect all verbs = roots + their conjuncts\n",
    "#         verbs = []\n",
    "#         for root in roots:\n",
    "#             verbs.append(root)\n",
    "#             verbs.extend([c for c in root.conjuncts if c.pos_ in (\"VERB\",\"AUX\")])\n",
    "\n",
    "#         # 3) Extract per-verb SVO\n",
    "#         for verb in verbs:\n",
    "#             # a) subjects: direct or inherited\n",
    "#             subs = [c for c in verb.children\n",
    "#                     if c.dep_ in (\"nsubj\",\"nsubjpass\") and c.pos_!=\"PRON\"]\n",
    "#             if not subs and verb.dep_==\"conj\":\n",
    "#                 # inherit from its head root\n",
    "#                 head = verb.head\n",
    "#                 subs = subj_map.get(head, [])\n",
    "\n",
    "#             if not subs:\n",
    "#                 continue\n",
    "\n",
    "#             # b) objects: attr/dobj/dative + prep→pobj\n",
    "#             objs = []\n",
    "#             for c in verb.children:\n",
    "#                 if c.dep_ in (\"dobj\",\"attr\",\"dative\") and c.pos_!=\"PRON\":\n",
    "#                     objs.append(c)\n",
    "#                 if c.dep_==\"prep\":\n",
    "#                     for pobj in c.children:\n",
    "#                         if pobj.dep_==\"pobj\" and pobj.pos_!=\"PRON\":\n",
    "#                             objs.append(pobj)\n",
    "#             if not objs:\n",
    "#                 continue\n",
    "\n",
    "#             # c) expand spans & record triples\n",
    "#             for subj in subs:\n",
    "#                 subj_text = subj.text  # head only\n",
    "#                 for obj in objs:\n",
    "#                     # drop pure numbers\n",
    "#                     if obj.pos_ == \"NUM\":\n",
    "#                         continue\n",
    "#                     # expand object to phrase\n",
    "#                     span = sent[obj.left_edge.i - sent.start : obj.right_edge.i - sent.start + 1]\n",
    "#                     obj_text = span.text\n",
    "#                     svos.append((subj_text, verb.lemma_, obj_text))\n",
    "\n",
    "#     return svos\n",
    "\n",
    "# # Quick sanity test\n",
    "# example = (\n",
    "#     \"Former Dons midfielder Sheerin, 39, has been player-manager at the Red Lichties \"\n",
    "#     \"since 2010 and replaces Neil Cooper at Pittodrie.\"\n",
    "# )\n",
    "# for sent in nlp(example).sents:\n",
    "#     print(\"SENTENCE:\", sent.text)\n",
    "#     print(\"→\", extract_svo_triples(sent.text))\n",
    "#     print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c18cedd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCE: Former Dons midfielder Sheerin, 39, has been player-manager at the Red Lichties since 2010 and replaces Neil Cooper at Pittodrie.\n",
      "\n",
      "→ [('Sheerin', 'replace', 'Pittodrie'), ('Sheerin', 'be', 'player-manager at the Red Lichties'), ('Sheerin', 'replace', 'Neil Cooper')]\n",
      "\n",
      "SENTENCE: Arbroath were relegated to Scottish League Two after finishing season 2013-14 bottom of League One.\n",
      "\n",
      "→ [('Arbroath', 'relegate', 'Scottish League')]\n",
      "\n",
      "SENTENCE: Aberdeen chief executive Duncan Fraser said boss Derek McInnes chose Sheerin after \"a comprehensive process\".\n",
      "\n",
      "→ [('McInnes', 'choose', 'a comprehensive process'), ('McInnes', 'choose', 'Sheerin')]\n",
      "\n",
      "SENTENCE: Speaking on the club's website, Fraser added: \"Despite wishing to retain Paul's services, Arbroath chairman John Christison was good to deal with and completely appreciated Paul's desire to come back to Aberdeen.\"\n",
      "Sheerin played for several Scottish clubs and also featured for Östersunds in Sweden before moving into coaching.\n",
      "→ [('Sheerin', 'feature', 'Östersunds in Sweden'), ('Christison', 'appreciate', \"Paul's desire to come back to Aberdeen\"), ('Sheerin', 'play', 'several Scottish clubs')]\n",
      "\n",
      "SENTENCE: He won a Scotland Under-21 cap in in the mid-1990s.\n",
      "\n",
      "→ []\n",
      "\n",
      "SENTENCE: Sheerin guided the Arbroath to their first national trophy in their history by winning the Scottish Third Division title and promotion in 2011.\n",
      "→ [('Sheerin', 'guide', 'the Arbroath')]\n",
      "\n",
      "SENTENCE: Former Dons midfielder Sheerin, 39, has been player-manager at the Red Lichties since 2010 and replaces Neil Cooper at Pittodrie.\n",
      "\n",
      "→ [('Sheerin', 'replace', 'Pittodrie'), ('Sheerin', 'be', 'player-manager at the Red Lichties'), ('Sheerin', 'replace', 'Neil Cooper')]\n",
      "\n",
      "SENTENCE: Arbroath were relegated to Scottish League Two after finishing season 2013-14 bottom of League One.\n",
      "\n",
      "→ [('Arbroath', 'relegate', 'Scottish League')]\n",
      "\n",
      "SENTENCE: Aberdeen chief executive Duncan Fraser said boss Derek McInnes chose Sheerin after \"a comprehensive process\".\n",
      "\n",
      "→ [('McInnes', 'choose', 'a comprehensive process'), ('McInnes', 'choose', 'Sheerin')]\n",
      "\n",
      "SENTENCE: Speaking on the club's website, Fraser added: \"Despite wishing to retain Paul's services, Arbroath chairman John Christison was good to deal with and completely appreciated Paul's desire to come back to Aberdeen.\"\n",
      "Sheerin played for several Scottish clubs and also featured for Östersunds in Sweden before moving into coaching.\n",
      "→ [('Sheerin', 'feature', 'Östersunds in Sweden'), ('Christison', 'appreciate', \"Paul's desire to come back to Aberdeen\"), ('Sheerin', 'play', 'several Scottish clubs')]\n",
      "\n",
      "SENTENCE: He won a Scotland Under-21 cap in in the mid-1990s.\n",
      "\n",
      "→ []\n",
      "\n",
      "SENTENCE: Sheerin guided the Arbroath to their first national trophy in their history by winning the Scottish Third Division title and promotion in 2011.\n",
      "→ [('Sheerin', 'guide', 'the Arbroath')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def extract_svo_triples(text: str):\n",
    "    \"\"\"\n",
    "    - Sentence‐level SVO\n",
    "    - Verbs: ROOT, ccomp, conj (VERB or AUX)\n",
    "    - Subjects: nsubj/nsubjpass (drop pronouns)\n",
    "    - Objects: dobj/attr/dative + first pobj under prep + ccomp handled separately\n",
    "    - Expand every object via its subtree\n",
    "    - Drop objects whose text starts with \"their \"\n",
    "    - Dedupe via a set\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    triples = set()\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        # 1) Collect candidate verbs\n",
    "        verbs = [t for t in sent\n",
    "                 if t.pos_ in (\"VERB\",\"AUX\") and t.dep_ in (\"ROOT\",\"ccomp\",\"conj\")]\n",
    "        # plus conjuncts of those\n",
    "        for v in list(verbs):\n",
    "            verbs.extend([c for c in v.conjuncts if c.pos_ in (\"VERB\",\"AUX\")])\n",
    "\n",
    "        for verb in verbs:\n",
    "            # 2) Subjects\n",
    "            subs = [c for c in verb.children\n",
    "                    if c.dep_ in (\"nsubj\",\"nsubjpass\") and c.pos_!=\"PRON\"]\n",
    "            # inherit for conj w/o own subs\n",
    "            if not subs and verb.dep_==\"conj\":\n",
    "                subs = [c for c in verb.head.children\n",
    "                        if c.dep_ in (\"nsubj\",\"nsubjpass\") and c.pos_!=\"PRON\"]\n",
    "            if not subs:\n",
    "                continue\n",
    "\n",
    "            # 3) Objects\n",
    "            objs = []\n",
    "            for c in verb.children:\n",
    "                if c.dep_ in (\"dobj\",\"attr\",\"dative\"):\n",
    "                    objs.append(c)\n",
    "                if c.dep_ == \"prep\":\n",
    "                    # first pobj under this prep\n",
    "                    for pobj in c.children:\n",
    "                        if pobj.dep_ == \"pobj\":\n",
    "                            objs.append(pobj)\n",
    "                            break\n",
    "                # handle clausal complements as separate SVOs\n",
    "                if c.dep_ == \"ccomp\":\n",
    "                    nested = extract_svo_triples(c.text)\n",
    "                    for (ns,nv,no) in nested:\n",
    "                        for sub in subs:\n",
    "                            triples.add((sub.text, verb.lemma_, f\"{nv} {no}\"))\n",
    "\n",
    "            if not objs:\n",
    "                continue\n",
    "\n",
    "            # 4) Build and filter triples\n",
    "            for sub in subs:\n",
    "                subj_text = sub.text\n",
    "                for obj in objs:\n",
    "                    if obj.pos_ in (\"PRON\",\"NUM\"):\n",
    "                        continue\n",
    "                    # full subtree span\n",
    "                    toks = list(obj.subtree)\n",
    "                    span = sent[toks[0].i - sent.start : toks[-1].i - sent.start + 1]\n",
    "                    obj_text = span.text\n",
    "                    if obj_text.lower().startswith(\"their \"):\n",
    "                        continue\n",
    "                    triples.add((subj_text, verb.lemma_, obj_text))\n",
    "\n",
    "    return list(triples)\n",
    "\n",
    "# Sanity‐check on your example:\n",
    "src = docs[str(factual_df.loc[10, \"bbcid\"])]\n",
    "for sent in nlp(src).sents:\n",
    "    print(\"SENTENCE:\", sent.text)\n",
    "    print(\"→\", extract_svo_triples(sent.text))\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Quick sanity check on your example:\n",
    "if __name__ == \"__main__\":\n",
    "    src = docs[str(factual_df.loc[10, \"bbcid\"])]\n",
    "    for sent in nlp(src).sents:\n",
    "        print(\"SENTENCE:\", sent.text)\n",
    "        print(\"→\", extract_svo_triples(sent.text))\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373cf817",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = docs[str(factual_df.loc[10, \"bbcid\"])]\n",
    "\n",
    "for sent in nlp(src).sents:\n",
    "    print(sent.text)\n",
    "    print(\"→\", extract_svo_triples(sent.text))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "944b69a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alireza/miniconda3/envs/NLP_project/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")  # :contentReference[oaicite:3]{index=3}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "020f52b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_entity(entity: str, source: str, threshold: float = 0.8) -> bool:\n",
    "    # exact substring?\n",
    "    if entity in source:\n",
    "        return True\n",
    "    # semantic match against each sentence\n",
    "    sents = [sent.text for sent in nlp(source).sents]\n",
    "    e_emb  = embedder.encode(entity, convert_to_tensor=True)\n",
    "    s_embs = embedder.encode(sents,   convert_to_tensor=True)\n",
    "    scores = util.cos_sim(e_emb, s_embs)\n",
    "    return scores.max().item() >= threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad9f0ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_triple(triple: tuple[str,str,str], source_triples: list, threshold: float = 0.8) -> bool:\n",
    "    # exact match?\n",
    "    if triple in source_triples:\n",
    "        return True\n",
    "    # semantic: compare string form to each source triple\n",
    "    txt   = \" \".join(triple)\n",
    "    src_txts = [\" \".join(t) for t in source_triples]\n",
    "    t_emb    = embedder.encode(txt,        convert_to_tensor=True)\n",
    "    src_embs = embedder.encode(src_txts,   convert_to_tensor=True)\n",
    "    scores   = util.cos_sim(t_emb, src_embs)\n",
    "    return scores.max().item() >= threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ca48cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_triple(triple: tuple[str,str,str], source_triples: list, threshold: float = 0.8) -> bool:\n",
    "    # exact match?\n",
    "    if triple in source_triples:\n",
    "        return True\n",
    "    # semantic: compare string form to each source triple\n",
    "    txt   = \" \".join(triple)\n",
    "    src_txts = [\" \".join(t) for t in source_triples]\n",
    "    t_emb    = embedder.encode(txt,        convert_to_tensor=True)\n",
    "    src_embs = embedder.encode(src_txts,   convert_to_tensor=True)\n",
    "    scores   = util.cos_sim(t_emb, src_embs)\n",
    "    return scores.max().item() >= threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1a92811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Former Dons midfielder Sheerin, 39, has been player-manager at the Red Lichties since 2010 and replaces Neil Cooper at Pittodrie.\n",
      "Arbroath were relegated to Scottish League Two after finishing season 2013-14 bottom of League One.\n",
      "Aberdeen chief executive Duncan Fraser said boss Derek McInnes chose Sheerin after \"a comprehensive process\".\n",
      "Speaking on the club's website, Fraser added: \"Despite wishing to retain Paul's services, Arbroath chairman John Christison was good to deal with and completely appreciated Paul's desire to come back to Aberdeen.\"\n",
      "Sheerin played for several Scottish clubs and also featured for Östersunds in Sweden before moving into coaching. He won a Scotland Under-21 cap in in the mid-1990s.\n",
      "Sheerin guided the Arbroath to their first national trophy in their history by winning the Scottish Third Division title and promotion in 2011.\n",
      "aberdeen have appointed arbroath boss paul sheerin as their new manager.\n",
      "FactScore: 0.0\n",
      "Errors: {\n",
      "  \"entity_hallucinations\": [\n",
      "    \"aberdeen\",\n",
      "    \"paul sheerin\"\n",
      "  ],\n",
      "  \"unsupported_triples\": [\n",
      "    [\n",
      "      \"aberdeen\",\n",
      "      \"appoint\",\n",
      "      \"sheerin\"\n",
      "    ]\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def compute_fact_score(summary: str, source: str, alpha: float = 0.5):\n",
    "    # 1. extract\n",
    "    ents_summ  = extract_entities(summary)\n",
    "    ents_src   = extract_entities(source)\n",
    "    triples_s  = extract_svo_triples(summary)\n",
    "    triples_src= extract_svo_triples(source)\n",
    "\n",
    "    # 2. match & collect unsupported\n",
    "    sup_ents, unsup_ents = 0, []\n",
    "    for e in ents_summ:\n",
    "        if match_entity(e, source):\n",
    "            sup_ents += 1\n",
    "        else:\n",
    "            unsup_ents.append(e)\n",
    "\n",
    "    sup_trips, unsup_trips = 0, []\n",
    "    for t in triples_s:\n",
    "        if match_triple(t, triples_src):\n",
    "            sup_trips += 1\n",
    "        else:\n",
    "            unsup_trips.append(t)\n",
    "\n",
    "    # 3. precision scores\n",
    "    ent_prec = sup_ents / len(ents_summ)     if ents_summ else 1.0\n",
    "    tri_prec = sup_trips / len(triples_s)    if triples_s else 1.0\n",
    "\n",
    "    # 4. combined FactScore\n",
    "    fact_score = alpha * ent_prec + (1-alpha) * tri_prec\n",
    "\n",
    "    # optional JSON error report\n",
    "    error_report = {\n",
    "        \"entity_hallucinations\": unsup_ents,\n",
    "        \"unsupported_triples\":  unsup_trips\n",
    "    }\n",
    "\n",
    "    return fact_score, error_report\n",
    "\n",
    "# Example usage\n",
    "# factual_df = pd.read_csv(\"../Data/factuality_annotations_xsum_summaries.csv\")\n",
    "\n",
    "# docs = extract_docs()\n",
    "src   = docs[str(factual_df['bbcid'][10])]\n",
    "summ  = factual_df['summary'][10]\n",
    "\n",
    "print(src)\n",
    "print(summ)\n",
    "score, report = compute_fact_score(summ, src, alpha=0.6)\n",
    "print(\"FactScore:\", score)\n",
    "print(\"Errors:\", json.dumps(report, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1053225d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
