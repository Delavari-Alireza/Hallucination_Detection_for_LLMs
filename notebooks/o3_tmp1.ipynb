{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "157ffacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alireza/miniconda3/envs/NLP_project/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading XSum… this takes ~1 min the first time.\n",
      "11,185 examples loaded.\n",
      "                                            document  \\\n",
      "0  France's Dubuisson carded a 67 to tie with ove...   \n",
      "1  France's Dubuisson carded a 67 to tie with ove...   \n",
      "2  France's Dubuisson carded a 67 to tie with ove...   \n",
      "\n",
      "                                             summary  label  \n",
      "0  rory mcilroy will take a one-shot lead into th...      0  \n",
      "1  rory mcilroy will take a one-shot lead into th...      0  \n",
      "2  rory mcilroy will take a one-shot lead into th...      0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "HALLU_CSV   = \"../Data/hallucination_annotations_xsum_summaries.csv\"\n",
    "FACTUAL_CSV = \"../Data/factuality_annotations_xsum_summaries.csv\"\n",
    "\n",
    "# 1) -----------------------------------------------------------------\n",
    "# Read the two annotation files\n",
    "faith_df   = pd.read_csv(HALLU_CSV)\n",
    "factual_df = pd.read_csv(FACTUAL_CSV)\n",
    "\n",
    "# 2) -----------------------------------------------------------------\n",
    "# Ensure a single gold label per summary\n",
    "if \"gold_is_factual\" not in faith_df.columns:\n",
    "    maj = (\n",
    "        (factual_df[\"is_factual\"] == \"yes\")\n",
    "        .groupby([factual_df.bbcid, factual_df.system])\n",
    "        .mean()                               # fraction \"yes\"\n",
    "        .ge(0.5)                              # majority vote\n",
    "        .reset_index()\n",
    "        .rename(columns={\"is_factual\": \"gold_is_factual\"})\n",
    "        .replace({True: \"yes\", False: \"no\"})\n",
    "    )\n",
    "    faith_df = faith_df.merge(maj, on=[\"bbcid\", \"system\"], how=\"left\")\n",
    "\n",
    "faith_df[\"label\"] = (faith_df[\"gold_is_factual\"] == \"yes\").astype(int)\n",
    "\n",
    "# 3) -----------------------------------------------------------------\n",
    "# Pull source articles from Hugging Face XSum\n",
    "print(\"Loading XSum… this takes ~1 min the first time.\")\n",
    "xsum = load_dataset(\"EdinburghNLP/xsum\")       # train/validation/test splits\n",
    "\n",
    "# Build a mapping id(int) -> document\n",
    "docs = {int(ex[\"id\"]): ex[\"document\"]\n",
    "        for split in xsum.values()\n",
    "        for ex in split}\n",
    "\n",
    "# 4) -----------------------------------------------------------------\n",
    "# Attach full article text\n",
    "faith_df[\"document\"] = faith_df[\"bbcid\"].map(docs)\n",
    "\n",
    "missing = faith_df[\"document\"].isna().sum()\n",
    "if missing:\n",
    "    raise ValueError(f\"{missing} BBC IDs in the CSV were not found in XSum\")\n",
    "\n",
    "# 5) -----------------------------------------------------------------\n",
    "# Keep only what we feed into the detector pipeline\n",
    "data = faith_df[[\"document\", \"summary\", \"label\"]]\n",
    "\n",
    "print(f\"{len(data):,} examples loaded.\")\n",
    "print(data.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95eec3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 6,711 rows (pos 6.68%)\n",
      "calib: 2,237 rows (pos 6.66%)\n",
      "test: 2,237 rows (pos 6.71%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, temp  = train_test_split(data, test_size=0.40,\n",
    "                                stratify=data.label, random_state=42)\n",
    "calib, test  = train_test_split(temp, test_size=0.50,\n",
    "                                stratify=temp.label, random_state=42)\n",
    "\n",
    "for df,name in [(train,\"train\"), (calib,\"calib\"), (test,\"test\")]:\n",
    "    print(f\"{name}: {len(df):,} rows (pos {df.label.mean():.2%})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66ae68b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown scheme for proxy URL URL('socks://127.0.0.1:10808/')",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mollama\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m chat\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mask_llm\u001b[39m(prompts, model=\u001b[33m\"\u001b[39m\u001b[33mllama3.2:1b\u001b[39m\u001b[33m\"\u001b[39m, **kw):\n\u001b[32m      4\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return a list of completions equal in length to prompts.\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/NLP_project/lib/python3.11/site-packages/ollama/__init__.py:40\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mollama\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_types\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      3\u001b[39m   ChatResponse,\n\u001b[32m      4\u001b[39m   EmbeddingsResponse,\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m   Tool,\n\u001b[32m     18\u001b[39m )\n\u001b[32m     20\u001b[39m __all__ = [\n\u001b[32m     21\u001b[39m   \u001b[33m'\u001b[39m\u001b[33mAsyncClient\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     22\u001b[39m   \u001b[33m'\u001b[39m\u001b[33mChatResponse\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     37\u001b[39m   \u001b[33m'\u001b[39m\u001b[33mTool\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     38\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m _client = \u001b[43mClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m generate = _client.generate\n\u001b[32m     43\u001b[39m chat = _client.chat\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/NLP_project/lib/python3.11/site-packages/ollama/_client.py:116\u001b[39m, in \u001b[36mClient.__init__\u001b[39m\u001b[34m(self, host, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, host: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m, **kwargs) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m   \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhttpx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mClient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/NLP_project/lib/python3.11/site-packages/ollama/_client.py:93\u001b[39m, in \u001b[36mBaseClient.__init__\u001b[39m\u001b[34m(self, client, host, follow_redirects, timeout, headers, **kwargs)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     76\u001b[39m   \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     77\u001b[39m   client,\n\u001b[32m   (...)\u001b[39m\u001b[32m     83\u001b[39m   **kwargs,\n\u001b[32m     84\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     85\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     86\u001b[39m \u001b[33;03m  Creates a httpx client. Default parameters are the same as those defined in httpx\u001b[39;00m\n\u001b[32m     87\u001b[39m \u001b[33;03m  except for the following:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     90\u001b[39m \u001b[33;03m  `kwargs` are passed to the httpx client.\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m   \u001b[38;5;28mself\u001b[39m._client = \u001b[43mclient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_parse_host\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mOLLAMA_HOST\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Lowercase all headers to ensure override\u001b[39;49;00m\n\u001b[32m     98\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m      \u001b[49m\u001b[43mk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mContent-Type\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mapplication/json\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mAccept\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mapplication/json\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mUser-Agent\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mollama-python/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43m__version__\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m (\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mplatform\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmachine\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mplatform\u001b[49m\u001b[43m.\u001b[49m\u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m) Python/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mplatform\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpython_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m      \u001b[49m\u001b[43m}\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/NLP_project/lib/python3.11/site-packages/httpx/_client.py:686\u001b[39m, in \u001b[36mClient.__init__\u001b[39m\u001b[34m(self, auth, params, headers, cookies, verify, cert, trust_env, http1, http2, proxy, mounts, timeout, follow_redirects, limits, max_redirects, event_hooks, base_url, transport, default_encoding)\u001b[39m\n\u001b[32m    680\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m    681\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mUsing http2=True, but the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mh2\u001b[39m\u001b[33m'\u001b[39m\u001b[33m package is not installed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    682\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMake sure to install httpx using `pip install httpx[http2]`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    683\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    685\u001b[39m allow_env_proxies = trust_env \u001b[38;5;129;01mand\u001b[39;00m transport \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m686\u001b[39m proxy_map = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_proxy_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproxy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_env_proxies\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[38;5;28mself\u001b[39m._transport = \u001b[38;5;28mself\u001b[39m._init_transport(\n\u001b[32m    689\u001b[39m     verify=verify,\n\u001b[32m    690\u001b[39m     cert=cert,\n\u001b[32m   (...)\u001b[39m\u001b[32m    695\u001b[39m     transport=transport,\n\u001b[32m    696\u001b[39m )\n\u001b[32m    697\u001b[39m \u001b[38;5;28mself\u001b[39m._mounts: \u001b[38;5;28mdict\u001b[39m[URLPattern, BaseTransport | \u001b[38;5;28;01mNone\u001b[39;00m] = {\n\u001b[32m    698\u001b[39m     URLPattern(key): \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    699\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m proxy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    709\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m key, proxy \u001b[38;5;129;01min\u001b[39;00m proxy_map.items()\n\u001b[32m    710\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/NLP_project/lib/python3.11/site-packages/httpx/_client.py:244\u001b[39m, in \u001b[36mBaseClient._get_proxy_map\u001b[39m\u001b[34m(self, proxy, allow_env_proxies)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m proxy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m allow_env_proxies:\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m{\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m            \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mProxy\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mget_environment_proxies\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m    248\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {}\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/NLP_project/lib/python3.11/site-packages/httpx/_client.py:245\u001b[39m, in \u001b[36m<dictcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m proxy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m allow_env_proxies:\n\u001b[32m    244\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m--> \u001b[39m\u001b[32m245\u001b[39m             key: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mProxy\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    246\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key, url \u001b[38;5;129;01min\u001b[39;00m get_environment_proxies().items()\n\u001b[32m    247\u001b[39m         }\n\u001b[32m    248\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {}\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/NLP_project/lib/python3.11/site-packages/httpx/_config.py:214\u001b[39m, in \u001b[36mProxy.__init__\u001b[39m\u001b[34m(self, url, ssl_context, auth, headers)\u001b[39m\n\u001b[32m    211\u001b[39m headers = Headers(headers)\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m url.scheme \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mhttp\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mhttps\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33msocks5\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33msocks5h\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnknown scheme for proxy URL \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m url.username \u001b[38;5;129;01mor\u001b[39;00m url.password:\n\u001b[32m    217\u001b[39m     \u001b[38;5;66;03m# Remove any auth credentials from the URL.\u001b[39;00m\n\u001b[32m    218\u001b[39m     auth = (url.username, url.password)\n",
      "\u001b[31mValueError\u001b[39m: Unknown scheme for proxy URL URL('socks://127.0.0.1:10808/')"
     ]
    }
   ],
   "source": [
    "from ollama import chat , Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "def ask_llm(prompts, model=\"llama3.2:1b\", **kw):\n",
    "    \"\"\"Return a list of completions equal in length to prompts.\"\"\"\n",
    "    out = []\n",
    "    for p in prompts:\n",
    "        res = client.chat(model=model, messages=[{\"role\":\"user\",\"content\":p}], **kw)\n",
    "        out.append(res[\"message\"][\"content\"])\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f5be8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, math\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tok_t5 = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "mod_t5 = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\").eval()\n",
    "\n",
    "@torch.no_grad()\n",
    "def inverse_ppl(texts, max_len=512, batch_size=8):\n",
    "    inv = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        enc = tok_t5(texts[i:i+batch_size], return_tensors=\"pt\", truncation=True,\n",
    "                     padding=True, max_length=max_len)\n",
    "        out = mod_t5(input_ids=enc[\"input_ids\"], attention_mask=enc[\"attention_mask\"],\n",
    "                     labels=enc[\"input_ids\"])\n",
    "        loss = out.loss\n",
    "        ppl  = torch.exp(loss)           # natural exp ≈ 2.718…\n",
    "        inv.extend( (1/ppl).cpu().tolist() )\n",
    "    return inv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66840eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch.nn.functional as F\n",
    "\n",
    "tok_nli = AutoTokenizer.from_pretrained(\"microsoft/deberta-large-mnli\")\n",
    "mod_nli = AutoModelForSequenceClassification.from_pretrained(\"microsoft/deberta-large-mnli\").eval()\n",
    "\n",
    "@torch.no_grad()\n",
    "def nli_faith(documents, summaries, batch_size=4):\n",
    "    \"\"\"Return 1 − P(contradiction) per pair.\"\"\"\n",
    "    scores = []\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        enc = tok_nli(\n",
    "            documents[i:i+batch_size],\n",
    "            summaries[i:i+batch_size],\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True, padding=True, max_length=512\n",
    "        )\n",
    "        logits = mod_nli(**enc).logits\n",
    "        probs  = F.softmax(logits, dim=-1)   # order: contradiction, neutral, entailment\n",
    "        scores.extend( (1 - probs[:,0]).cpu().tolist() )\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5c6981d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_true(documents, summaries, judge=\"qwen:7b\", batch_size=2):\n",
    "    prompts = [\n",
    "        f\"You are a fact-checker. Respond only with 'True' or 'False'.\\n\"\n",
    "        f\"Document:\\n{doc}\\n---\\nSummary:\\n{summ}\\n---\\nIs the summary factual?\"\n",
    "        for doc,summ in zip(documents,summaries)\n",
    "    ]\n",
    "    outs = ask_llm(prompts, model=judge, stream=False)\n",
    "    return [ 1.0 if o.strip().lower().startswith(\"true\") else 0.0 for o in outs ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "173e5367",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m splits = \u001b[38;5;28mdict\u001b[39m(train=train, calib=calib, test=test)\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name,df \u001b[38;5;129;01min\u001b[39;00m splits.items():\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     df[\u001b[33m\"\u001b[39m\u001b[33mscore_invppl\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43minverse_ppl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msummary\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     df[\u001b[33m\"\u001b[39m\u001b[33mscore_nli\u001b[39m\u001b[33m\"\u001b[39m]    = nli_faith(df[\u001b[33m\"\u001b[39m\u001b[33mdocument\u001b[39m\u001b[33m\"\u001b[39m].tolist(), df[\u001b[33m\"\u001b[39m\u001b[33msummary\u001b[39m\u001b[33m\"\u001b[39m].tolist())\n\u001b[32m      5\u001b[39m     df[\u001b[33m\"\u001b[39m\u001b[33mscore_ptrue\u001b[39m\u001b[33m\"\u001b[39m]  = p_true(df[\u001b[33m\"\u001b[39m\u001b[33mdocument\u001b[39m\u001b[33m\"\u001b[39m].tolist(), df[\u001b[33m\"\u001b[39m\u001b[33msummary\u001b[39m\u001b[33m\"\u001b[39m].tolist())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/NLP_project/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36minverse_ppl\u001b[39m\u001b[34m(texts, max_len, batch_size)\u001b[39m\n\u001b[32m     15\u001b[39m     loss = out.loss\n\u001b[32m     16\u001b[39m     ppl  = torch.exp(loss)           \u001b[38;5;66;03m# natural exp ≈ 2.718…\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     \u001b[43minv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m/\u001b[49m\u001b[43mppl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m inv\n",
      "\u001b[31mTypeError\u001b[39m: 'float' object is not iterable"
     ]
    }
   ],
   "source": [
    "splits = dict(train=train, calib=calib, test=test)\n",
    "for name,df in splits.items():\n",
    "    df[\"score_invppl\"] = inverse_ppl(df[\"summary\"].tolist())\n",
    "    df[\"score_nli\"]    = nli_faith(df[\"document\"].tolist(), df[\"summary\"].tolist())\n",
    "    df[\"score_ptrue\"]  = p_true(df[\"document\"].tolist(), df[\"summary\"].tolist())\n",
    "    df.to_parquet(f\"xsum_{name}_scores.pq\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed64c598",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'xsum_calib_scores.pq'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m     ir.fit(x, y)\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ir\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m calib_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mxsum_calib_scores.pq\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m calibrators = {}\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mscore_invppl\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mscore_nli\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mscore_ptrue\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/NLP_project/lib/python3.11/site-packages/pandas/io/parquet.py:669\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    666\u001b[39m     use_nullable_dtypes = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    667\u001b[39m check_dtype_backend(dtype_backend)\n\u001b[32m--> \u001b[39m\u001b[32m669\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/NLP_project/lib/python3.11/site-packages/pandas/io/parquet.py:258\u001b[39m, in \u001b[36mPyArrowImpl.read\u001b[39m\u001b[34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m manager == \u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    257\u001b[39m     to_pandas_kwargs[\u001b[33m\"\u001b[39m\u001b[33msplit_blocks\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m path_or_handle, handles, filesystem = \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    265\u001b[39m     pa_table = \u001b[38;5;28mself\u001b[39m.api.parquet.read_table(\n\u001b[32m    266\u001b[39m         path_or_handle,\n\u001b[32m    267\u001b[39m         columns=columns,\n\u001b[32m   (...)\u001b[39m\u001b[32m    270\u001b[39m         **kwargs,\n\u001b[32m    271\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/NLP_project/lib/python3.11/site-packages/pandas/io/parquet.py:141\u001b[39m, in \u001b[36m_get_path_or_handle\u001b[39m\u001b[34m(path, fs, storage_options, mode, is_dir)\u001b[39m\n\u001b[32m    131\u001b[39m handles = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    133\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[32m    134\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[32m   (...)\u001b[39m\u001b[32m    139\u001b[39m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[32m    140\u001b[39m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m     fs = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    145\u001b[39m     path_or_handle = handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/NLP_project/lib/python3.11/site-packages/pandas/io/common.py:882\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n\u001b[32m    883\u001b[39m     handles.append(handle)\n\u001b[32m    885\u001b[39m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'xsum_calib_scores.pq'"
     ]
    }
   ],
   "source": [
    "from sklearn.isotonic import IsotonicRegression\n",
    "import numpy as np\n",
    "\n",
    "def platt(x, y):\n",
    "    # x = raw score (0-1), y = 0/1 label\n",
    "    ir = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "    ir.fit(x, y)\n",
    "    return ir\n",
    "\n",
    "calib_df = pd.read_parquet(\"xsum_calib_scores.pq\")\n",
    "calibrators = {}\n",
    "for col in [\"score_invppl\", \"score_nli\", \"score_ptrue\"]:\n",
    "    calibrators[col] = platt(calib_df[col].values, calib_df.label.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48406f79",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'calibrators' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjoblib\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m      2\u001b[39m os.makedirs(\u001b[33m\"\u001b[39m\u001b[33martifacts\u001b[39m\u001b[33m\"\u001b[39m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m joblib.dump(\u001b[43mcalibrators\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33martifacts/calibrators.pkl\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'calibrators' is not defined"
     ]
    }
   ],
   "source": [
    "import joblib, os, json\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "joblib.dump(calibrators, \"artifacts/calibrators.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62cd63b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'calib_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m     X = logit(np.vstack(feats).T.clip(\u001b[32m1e-4\u001b[39m, \u001b[32m1\u001b[39m-\u001b[32m1e-4\u001b[39m))\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m X\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m X_cal   = make_X(\u001b[43mcalib_df\u001b[49m)\n\u001b[32m     13\u001b[39m y_cal   = calib_df.label.values\n\u001b[32m     15\u001b[39m meta = LogisticRegression(max_iter=\u001b[32m200\u001b[39m, class_weight=\u001b[33m\"\u001b[39m\u001b[33mbalanced\u001b[39m\u001b[33m\"\u001b[39m).fit(X_cal, y_cal)\n",
      "\u001b[31mNameError\u001b[39m: name 'calib_df' is not defined"
     ]
    }
   ],
   "source": [
    "from scipy.special import logit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def make_X(df):\n",
    "    feats = []\n",
    "    for col in [\"score_invppl\",\"score_nli\",\"score_ptrue\"]:\n",
    "        p = calibrators[col].predict(df[col].values)\n",
    "        feats.append(p)\n",
    "    X = logit(np.vstack(feats).T.clip(1e-4, 1-1e-4))\n",
    "    return X\n",
    "\n",
    "X_cal   = make_X(calib_df)\n",
    "y_cal   = calib_df.label.values\n",
    "\n",
    "meta = LogisticRegression(max_iter=200, class_weight=\"balanced\").fit(X_cal, y_cal)\n",
    "joblib.dump(meta, \"artifacts/meta.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e61b6bbd",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'xsum_test_scores.pq'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m f1_score, brier_score_loss, roc_auc_score\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m test_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mxsum_test_scores.pq\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m X_test  = make_X(test_df)\n\u001b[32m      5\u001b[39m p_test  = meta.predict_proba(X_test)[:,\u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/NLP_project/lib/python3.11/site-packages/pandas/io/parquet.py:669\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    666\u001b[39m     use_nullable_dtypes = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    667\u001b[39m check_dtype_backend(dtype_backend)\n\u001b[32m--> \u001b[39m\u001b[32m669\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/NLP_project/lib/python3.11/site-packages/pandas/io/parquet.py:258\u001b[39m, in \u001b[36mPyArrowImpl.read\u001b[39m\u001b[34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m manager == \u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    257\u001b[39m     to_pandas_kwargs[\u001b[33m\"\u001b[39m\u001b[33msplit_blocks\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m path_or_handle, handles, filesystem = \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    265\u001b[39m     pa_table = \u001b[38;5;28mself\u001b[39m.api.parquet.read_table(\n\u001b[32m    266\u001b[39m         path_or_handle,\n\u001b[32m    267\u001b[39m         columns=columns,\n\u001b[32m   (...)\u001b[39m\u001b[32m    270\u001b[39m         **kwargs,\n\u001b[32m    271\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/NLP_project/lib/python3.11/site-packages/pandas/io/parquet.py:141\u001b[39m, in \u001b[36m_get_path_or_handle\u001b[39m\u001b[34m(path, fs, storage_options, mode, is_dir)\u001b[39m\n\u001b[32m    131\u001b[39m handles = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    133\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[32m    134\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[32m   (...)\u001b[39m\u001b[32m    139\u001b[39m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[32m    140\u001b[39m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m     fs = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    145\u001b[39m     path_or_handle = handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/NLP_project/lib/python3.11/site-packages/pandas/io/common.py:882\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n\u001b[32m    883\u001b[39m     handles.append(handle)\n\u001b[32m    885\u001b[39m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'xsum_test_scores.pq'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, brier_score_loss, roc_auc_score\n",
    "\n",
    "test_df = pd.read_parquet(\"xsum_test_scores.pq\")\n",
    "X_test  = make_X(test_df)\n",
    "p_test  = meta.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(\"F1 @ 0.5:\",   f1_score(test_df.label, p_test > 0.5))\n",
    "print(\"Brier:\",       brier_score_loss(test_df.label, p_test))\n",
    "print(\"AUROC:\",       roc_auc_score(test_df.label, p_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aab78451",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'artifacts/calibrators.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mspecial\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m calibrators = \u001b[43mjoblib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43martifacts/calibrators.pkl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m meta        = joblib.load(\u001b[33m\"\u001b[39m\u001b[33martifacts/meta.pkl\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhallucination_guard\u001b[39m(doc, summ, thr=\u001b[32m0.5\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/NLP_project/lib/python3.11/site-packages/joblib/numpy_pickle.py:735\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(filename, mmap_mode, ensure_native_byte_order)\u001b[39m\n\u001b[32m    733\u001b[39m         obj = _unpickle(fobj, ensure_native_byte_order=ensure_native_byte_order)\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m735\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    736\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m _validate_fileobject_and_memmap(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m (\n\u001b[32m    737\u001b[39m             fobj,\n\u001b[32m    738\u001b[39m             validated_mmap_mode,\n\u001b[32m    739\u001b[39m         ):\n\u001b[32m    740\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    741\u001b[39m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[32m    742\u001b[39m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[32m    743\u001b[39m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'artifacts/calibrators.pkl'"
     ]
    }
   ],
   "source": [
    "import scipy.special as sp\n",
    "calibrators = joblib.load(\"artifacts/calibrators.pkl\")\n",
    "meta        = joblib.load(\"artifacts/meta.pkl\")\n",
    "\n",
    "def hallucination_guard(doc, summ, thr=0.5):\n",
    "    raw = {\n",
    "        \"score_invppl\": inverse_ppl([summ])[0],\n",
    "        \"score_nli\":    nli_faith([doc], [summ])[0],\n",
    "        \"score_ptrue\":  p_true([doc], [summ])[0],\n",
    "    }\n",
    "    xs = [ calibrators[k].predict([raw[k]])[0] for k in raw ]\n",
    "    p  = meta.predict_proba( sp.logit(np.array(xs).reshape(1,-1)) )[0,1]\n",
    "    return {\"p_hallucination\": float(p),\n",
    "            \"is_hallucinated\": bool(p > thr),\n",
    "            \"breakdown\": raw}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
