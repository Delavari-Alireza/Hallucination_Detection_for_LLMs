{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89e276d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alireza/miniconda3/envs/NLP_project/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import torch\n",
    "import numpy as np\n",
    "from sentence_transformers import CrossEncoder, SentenceTransformer, util\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "356eea8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) NLI cross-encoder\n",
    "nli_model = CrossEncoder(\"cross-encoder/nli-deberta-v3-base\")\n",
    "\n",
    "# 2) SBERT for topic embeddings\n",
    "topic_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# 3) spaCy for sentence splitting\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bab6c4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_semantic_entailment(source: str, summary: str) -> float:\n",
    "    \"\"\"\n",
    "    Returns ScoreNLI = max_{s∈src_sents, h∈sum_sents}( P_entailment - P_contradiction )\n",
    "    using cross-encoder/nli-deberta-v3-base.\n",
    "    Scores range in [-1,1]: +1 = strong entailment, -1 = strong contradiction.\n",
    "    \"\"\"\n",
    "    # 1. split into sentences\n",
    "    src_sents  = [sent.text for sent in nlp(source).sents]\n",
    "    sum_sents  = [sent.text for sent in nlp(summary).sents]\n",
    "\n",
    "    best_score = -1.0  # worst-case\n",
    "    for prem in src_sents:\n",
    "        for hyp in sum_sents:\n",
    "            # get logits for [contradiction, neutral, entailment]\n",
    "            logits = nli_model.predict([(prem, hyp)])[0]\n",
    "            probs  = torch.softmax(torch.tensor(logits), dim=0).numpy()\n",
    "            p_contradict, p_neutral, p_entail = probs\n",
    "            score = p_entail - p_contradict\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "\n",
    "    return best_score\n",
    "\n",
    "\n",
    "def compute_topic_drift(source: str, summary: str) -> float:\n",
    "    \"\"\"\n",
    "    Returns TopicDrift = 1 - cosine( embed(source), embed(summary) )\n",
    "    where embeddings come from all-MiniLM-L6-v2.\n",
    "    Drift in [0,2], but practically in [0,1].\n",
    "    \"\"\"\n",
    "    emb_src = topic_model.encode(source,  convert_to_tensor=True)\n",
    "    emb_sum = topic_model.encode(summary, convert_to_tensor=True)\n",
    "    cos_sim = util.cos_sim(emb_src, emb_sum).item()\n",
    "    drift   = 1.0 - cos_sim\n",
    "    return drift\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68c3e8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Entailment Score: 0.978  (+1=entail, -1=contra)\n",
      "Topic Drift Score:         0.452  (0=on-topic, 1=off-topic)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    example_src = (\n",
    "        \"The visitors led briefly through Vasil Lobzhanidze's early try, \"\n",
    "        \"but Scotland raced ahead ... and got their reward.\"\n",
    "    )\n",
    "    example_sum = \"Scotland dominated after an early Georgian try and ran out convincing winners.\"\n",
    "\n",
    "    nli_score  = compute_semantic_entailment(example_src, example_sum)\n",
    "    drift_score = compute_topic_drift(example_src, example_sum)\n",
    "\n",
    "    print(f\"Semantic Entailment Score: {nli_score:.3f}  (+1=entail, -1=contra)\")\n",
    "    print(f\"Topic Drift Score:         {drift_score:.3f}  (0=on-topic, 1=off-topic)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfc1769e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def extract_docs():\n",
    "    from datasets import load_dataset\n",
    "    xsum = load_dataset(\"EdinburghNLP/xsum\")  # :contentReference[oaicite:1]{index=1}\n",
    "    docs = {ex[\"id\"]: ex[\"document\"] for split in xsum.values()  # :contentReference[oaicite:2]{index=2}\n",
    "            for ex in split}\n",
    "    return docs\n",
    "import pandas as pd\n",
    "factual_df = pd.read_csv(\"../Data/factuality_annotations_xsum_summaries.csv\")\n",
    "\n",
    "df = factual_df.sample(\n",
    "    frac=1, random_state=42\n",
    ").reset_index(drop=True)\n",
    "train_df = df.iloc[:500]\n",
    "\n",
    "docs = extract_docs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18fc19aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring: 100%|██████████| 500/500 [07:38<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       semantic_entailment  topic_drift\n",
      "count           500.000000   500.000000\n",
      "mean              0.940750     0.485253\n",
      "std               0.305933     0.135311\n",
      "min              -0.999059     0.137819\n",
      "25%               0.997383     0.383937\n",
      "50%               0.998993     0.470322\n",
      "75%               0.999499     0.578487\n",
      "max               0.999781     0.990211\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Assume you already have:\n",
    "# - `train_df` with columns [\"bbcid\", \"summary\", \"system\", \"is_factual\"]\n",
    "# - `docs` dict mapping BBC IDs to source texts\n",
    "# - compute_semantic_entailment and compute_topic_drift defined\n",
    "\n",
    "# Prepare storage\n",
    "records = []\n",
    "\n",
    "for _, row in tqdm(train_df.iterrows(), total=len(train_df), desc=\"Scoring\"):\n",
    "    bbcid   = str(row[\"bbcid\"])\n",
    "    src     = docs[bbcid]\n",
    "    summ    = row[\"summary\"]\n",
    "\n",
    "    se_score = compute_semantic_entailment(src, summ)\n",
    "    td_score = compute_topic_drift(src, summ)\n",
    "\n",
    "    records.append({\n",
    "        \"bbcid\":              bbcid,\n",
    "        \"system\":             row[\"system\"],\n",
    "        \"gold_is_factual\":    row[\"is_factual\"],\n",
    "        \"semantic_entailment\": se_score,\n",
    "        \"topic_drift\":         td_score,\n",
    "    })\n",
    "\n",
    "# Build a DataFrame\n",
    "scores_df = pd.DataFrame(records)\n",
    "\n",
    "# Quick look\n",
    "print(scores_df[[\"semantic_entailment\",\"topic_drift\"]].describe())\n",
    "\n",
    "# Save to CSV if you like\n",
    "scores_df.to_csv(\"xsum_nli_topic_scores.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe03ccd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
